{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: aiohttp==3.8.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 1)) (3.8.3)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Cannot uninstall 'entrypoints'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: aiosignal==1.3.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: appnope==0.1.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 3)) (0.1.3)\n",
      "Requirement already satisfied: asttokens==2.2.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 4)) (2.2.1)\n",
      "Requirement already satisfied: async-timeout==4.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 5)) (4.0.2)\n",
      "Requirement already satisfied: attrs==22.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 6)) (22.2.0)\n",
      "Requirement already satisfied: backcall==0.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 7)) (0.2.0)\n",
      "Requirement already satisfied: beautifulsoup4==4.11.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 8)) (4.11.1)\n",
      "Requirement already satisfied: blobfile==2.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 9)) (2.0.1)\n",
      "Requirement already satisfied: bs4==0.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 10)) (0.0.1)\n",
      "Requirement already satisfied: certifi==2022.12.7 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 11)) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer==2.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 12)) (2.1.1)\n",
      "Requirement already satisfied: comm==0.1.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 13)) (0.1.2)\n",
      "Requirement already satisfied: contourpy==1.0.7 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 14)) (1.0.7)\n",
      "Requirement already satisfied: cycler==0.11.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 15)) (0.11.0)\n",
      "Requirement already satisfied: debugpy==1.6.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 16)) (1.6.5)\n",
      "Requirement already satisfied: decorator==5.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 17)) (5.1.1)\n",
      "Requirement already satisfied: docopt==0.6.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 18)) (0.6.2)\n",
      "Collecting entrypoints==0.4\n",
      "  Using cached entrypoints-0.4-py3-none-any.whl (5.3 kB)\n",
      "Collecting executing==1.2.0\n",
      "  Using cached executing-1.2.0-py2.py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: filelock==3.9.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 21)) (3.9.0)\n",
      "Collecting fonttools==4.38.0\n",
      "  Using cached fonttools-4.38.0-py3-none-any.whl (965 kB)\n",
      "Requirement already satisfied: frozenlist==1.3.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 23)) (1.3.3)\n",
      "Collecting huggingface-hub==0.11.1\n",
      "  Using cached huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
      "Requirement already satisfied: idna==3.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 25)) (3.4)\n",
      "Collecting ipykernel==6.20.1\n",
      "  Using cached ipykernel-6.20.1-py3-none-any.whl (149 kB)\n",
      "Collecting ipython==8.8.0\n",
      "  Using cached ipython-8.8.0-py3-none-any.whl (775 kB)\n",
      "Collecting jedi==0.18.2\n",
      "  Using cached jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
      "Collecting joblib==1.2.0\n",
      "  Using cached joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "Collecting jupyter_client==7.4.8\n",
      "  Using cached jupyter_client-7.4.8-py3-none-any.whl (133 kB)\n",
      "Collecting jupyter_core==5.1.3\n",
      "  Using cached jupyter_core-5.1.3-py3-none-any.whl (93 kB)\n",
      "Collecting kiwisolver==1.4.4\n",
      "  Using cached kiwisolver-1.4.4-cp38-cp38-win_amd64.whl (55 kB)\n",
      "Requirement already satisfied: lxml==4.9.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 33)) (4.9.2)\n",
      "Collecting matplotlib==3.6.3\n",
      "  Using cached matplotlib-3.6.3-cp38-cp38-win_amd64.whl (7.2 MB)\n",
      "Collecting matplotlib-inline==0.1.6\n",
      "  Using cached matplotlib_inline-0.1.6-py3-none-any.whl (9.4 kB)\n",
      "Requirement already satisfied: multidict==6.0.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 36)) (6.0.4)\n",
      "Collecting nest-asyncio==1.5.6\n",
      "  Using cached nest_asyncio-1.5.6-py3-none-any.whl (5.2 kB)\n",
      "Collecting numpy==1.24.1\n",
      "  Using cached numpy-1.24.1-cp38-cp38-win_amd64.whl (14.9 MB)\n",
      "Processing c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\2f\\9c\\55\\95d3609ccfc463eeffb96d50c756f1f1899453b85e92021a0a\\openai-0.26.1-py3-none-any.whl\n",
      "Collecting packaging==23.0\n",
      "  Using cached packaging-23.0-py3-none-any.whl (42 kB)\n",
      "Collecting pandas==1.5.2\n",
      "  Using cached pandas-1.5.2-cp38-cp38-win_amd64.whl (11.0 MB)\n",
      "Collecting parso==0.8.3\n",
      "  Using cached parso-0.8.3-py2.py3-none-any.whl (100 kB)\n",
      "Requirement already satisfied: pexpect==4.8.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 43)) (4.8.0)\n",
      "Requirement already satisfied: pickleshare==0.7.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 44)) (0.7.5)\n",
      "Collecting Pillow==9.4.0\n",
      "  Using cached Pillow-9.4.0-cp38-cp38-win_amd64.whl (2.5 MB)\n",
      "Collecting pipreqs==0.4.11\n",
      "  Using cached pipreqs-0.4.11-py2.py3-none-any.whl (32 kB)\n",
      "Collecting platformdirs==2.6.2\n",
      "  Using cached platformdirs-2.6.2-py3-none-any.whl (14 kB)\n",
      "Collecting plotly==5.12.0\n",
      "  Using cached plotly-5.12.0-py2.py3-none-any.whl (15.2 MB)\n",
      "Collecting prompt-toolkit==3.0.36\n",
      "  Using cached prompt_toolkit-3.0.36-py3-none-any.whl (386 kB)\n",
      "Collecting psutil==5.9.4\n",
      "  Using cached psutil-5.9.4-cp36-abi3-win_amd64.whl (252 kB)\n",
      "Collecting ptyprocess==0.7.0\n",
      "  Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)\n",
      "Collecting pure-eval==0.2.2\n",
      "  Using cached pure_eval-0.2.2-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: pycryptodomex==3.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 53)) (3.17)\n",
      "Collecting Pygments==2.14.0\n",
      "  Using cached Pygments-2.14.0-py3-none-any.whl (1.1 MB)"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting pyparsing==3.0.9\n",
      "  Using cached pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "Requirement already satisfied: python-dateutil==2.8.2 in c:\\users\\user\\appdata\\roaming\\python\\python38\\site-packages (from -r requirements.txt (line 56)) (2.8.2)\n",
      "Collecting pytz==2022.7.1\n",
      "  Using cached pytz-2022.7.1-py2.py3-none-any.whl (499 kB)\n",
      "Collecting PyYAML==6.0\n",
      "  Using cached PyYAML-6.0-cp38-cp38-win_amd64.whl (155 kB)\n",
      "Collecting pyzmq==24.0.1\n",
      "  Using cached pyzmq-24.0.1-cp38-cp38-win_amd64.whl (998 kB)\n",
      "Collecting regex==2022.10.31\n",
      "  Using cached regex-2022.10.31-cp38-cp38-win_amd64.whl (267 kB)\n",
      "Collecting requests==2.28.1\n",
      "  Using cached requests-2.28.1-py3-none-any.whl (62 kB)\n",
      "Collecting scikit-learn==1.2.0\n",
      "  Using cached scikit_learn-1.2.0-cp38-cp38-win_amd64.whl (8.2 MB)\n",
      "Collecting scipy==1.10.0\n",
      "  Using cached scipy-1.10.0-cp38-cp38-win_amd64.whl (42.2 MB)\n",
      "Requirement already satisfied: six==1.16.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 64)) (1.16.0)\n",
      "Requirement already satisfied: soupsieve==2.3.2.post1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 65)) (2.3.2.post1)\n",
      "Collecting stack-data==0.6.2\n",
      "  Using cached stack_data-0.6.2-py3-none-any.whl (24 kB)\n",
      "Collecting tenacity==8.1.0\n",
      "  Using cached tenacity-8.1.0-py3-none-any.whl (23 kB)\n",
      "Collecting threadpoolctl==3.1.0\n",
      "  Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting tiktoken==0.1.2\n",
      "  Using cached tiktoken-0.1.2-cp38-cp38-win_amd64.whl (575 kB)\n",
      "Collecting tokenizers==0.13.2\n",
      "  Using cached tokenizers-0.13.2-cp38-cp38-win_amd64.whl (3.3 MB)\n",
      "Collecting tornado==6.2\n",
      "  Using cached tornado-6.2-cp37-abi3-win_amd64.whl (425 kB)\n",
      "Collecting tqdm==4.64.1\n",
      "  Using cached tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: traitlets==5.8.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 73)) (5.8.1)\n",
      "Collecting transformers==4.25.1\n",
      "  Using cached transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
      "Requirement already satisfied: typing_extensions==4.4.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 75)) (4.4.0)\n",
      "Requirement already satisfied: urllib3==1.26.13 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 76)) (1.26.13)\n",
      "Requirement already satisfied: wcwidth==0.2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 77)) (0.2.5)\n",
      "Collecting yarg==0.1.9\n",
      "  Using cached yarg-0.1.9-py2.py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: yarl==1.8.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 79)) (1.8.2)\n",
      "Requirement already satisfied: colorama; sys_platform == \"win32\" in c:\\users\\user\\anaconda3\\lib\\site-packages (from ipython==8.8.0->-r requirements.txt (line 27)) (0.4.4)\n",
      "Requirement already satisfied: pywin32>=1.0; sys_platform == \"win32\" and platform_python_implementation != \"PyPy\" in c:\\users\\user\\anaconda3\\lib\\site-packages (from jupyter_core==5.1.3->-r requirements.txt (line 31)) (227)\n",
      "Installing collected packages: entrypoints, executing, fonttools, packaging, PyYAML, requests, tqdm, huggingface-hub, pyzmq, psutil, tornado, platformdirs, jupyter-core, nest-asyncio, jupyter-client, matplotlib-inline, prompt-toolkit, pure-eval, stack-data, Pygments, parso, jedi, ipython, ipykernel, joblib, kiwisolver, pyparsing, Pillow, numpy, matplotlib, openai, pytz, pandas, yarg, pipreqs, tenacity, plotly, ptyprocess, regex, scipy, threadpoolctl, scikit-learn, tiktoken, tokenizers, transformers\n",
      "  Attempting uninstall: entrypoints\n",
      "    Found existing installation: entrypoints 0.3\n",
      "Requirement already satisfied: tiktoken in c:\\users\\user\\anaconda3\\lib\\site-packages (0.4.0)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tiktoken) (2023.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (1.26.13)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\user\\anaconda3\\lib\\site-packages (0.27.8)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\user\\anaconda3\\lib\\site-packages (from openai) (3.8.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\lib\\site-packages (from openai) (4.50.2)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\user\\anaconda3\\lib\\site-packages (from openai) (2.31.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp->openai) (22.2.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp->openai) (2.1.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.8.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (1.26.13)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (3.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: plotly in c:\\users\\user\\anaconda3\\lib\\site-packages (5.15.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from plotly) (8.2.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\anaconda3\\lib\\site-packages (from plotly) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from packaging->plotly) (2.4.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: numpy in c:\\users\\user\\anaconda3\\lib\\site-packages (1.24.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade numpy --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai[embeddings] in c:\\users\\user\\anaconda3\\lib\\site-packages (0.27.8)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\lib\\site-packages (from openai[embeddings]) (4.50.2)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\user\\anaconda3\\lib\\site-packages (from openai[embeddings]) (2.31.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\user\\anaconda3\\lib\\site-packages (from openai[embeddings]) (3.8.3)\n",
      "Requirement already satisfied: scipy; extra == \"embeddings\" in c:\\users\\user\\anaconda3\\lib\\site-packages (from openai[embeddings]) (1.5.2)\n",
      "Requirement already satisfied: matplotlib; extra == \"embeddings\" in c:\\users\\user\\anaconda3\\lib\\site-packages (from openai[embeddings]) (3.3.2)\n",
      "Requirement already satisfied: pandas-stubs>=1.1.0.11; extra == \"embeddings\" in c:\\users\\user\\appdata\\roaming\\python\\python38\\site-packages (from openai[embeddings]) (2.0.2.230605)\n",
      "Requirement already satisfied: plotly; extra == \"embeddings\" in c:\\users\\user\\anaconda3\\lib\\site-packages (from openai[embeddings]) (5.15.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2; extra == \"embeddings\" in c:\\users\\user\\anaconda3\\lib\\site-packages (from openai[embeddings]) (1.2.2)\n",
      "Requirement already satisfied: numpy; extra == \"embeddings\" in c:\\users\\user\\anaconda3\\lib\\site-packages (from openai[embeddings]) (1.24.4)\n",
      "Requirement already satisfied: pandas>=1.2.3; extra == \"embeddings\" in c:\\users\\user\\appdata\\roaming\\python\\python38\\site-packages (from openai[embeddings]) (2.0.2)\n",
      "Requirement already satisfied: openpyxl>=3.0.7; extra == \"embeddings\" in c:\\users\\user\\anaconda3\\lib\\site-packages (from openai[embeddings]) (3.1.2)\n",
      "Requirement already satisfied: tenacity>=8.0.1; extra == \"embeddings\" in c:\\users\\user\\anaconda3\\lib\\site-packages (from openai[embeddings]) (8.2.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.20->openai[embeddings]) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.20->openai[embeddings]) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.20->openai[embeddings]) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.20->openai[embeddings]) (1.26.13)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp->openai[embeddings]) (22.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp->openai[embeddings]) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp->openai[embeddings]) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp->openai[embeddings]) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp->openai[embeddings]) (1.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp->openai[embeddings]) (1.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\user\\appdata\\roaming\\python\\python38\\site-packages (from matplotlib; extra == \"embeddings\"->openai[embeddings]) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib; extra == \"embeddings\"->openai[embeddings]) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib; extra == \"embeddings\"->openai[embeddings]) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib; extra == \"embeddings\"->openai[embeddings]) (8.0.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib; extra == \"embeddings\"->openai[embeddings]) (2.4.7)\n",
      "Requirement already satisfied: types-pytz>=2022.1.1 in c:\\users\\user\\appdata\\roaming\\python\\python38\\site-packages (from pandas-stubs>=1.1.0.11; extra == \"embeddings\"->openai[embeddings]) (2023.3.0.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\anaconda3\\lib\\site-packages (from plotly; extra == \"embeddings\"->openai[embeddings]) (21.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.2; extra == \"embeddings\"->openai[embeddings]) (2.1.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.2; extra == \"embeddings\"->openai[embeddings]) (1.3.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas>=1.2.3; extra == \"embeddings\"->openai[embeddings]) (2020.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\user\\appdata\\roaming\\python\\python38\\site-packages (from pandas>=1.2.3; extra == \"embeddings\"->openai[embeddings]) (2023.3)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\user\\anaconda3\\lib\\site-packages (from openpyxl>=3.0.7; extra == \"embeddings\"->openai[embeddings]) (1.0.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from python-dateutil>=2.1->matplotlib; extra == \"embeddings\"->openai[embeddings]) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "pip install openai[embeddings] --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import requests\n",
    "import re\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import deque\n",
    "from html.parser import HTMLParser\n",
    "from urllib.parse import urlparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import openai\n",
    "import numpy as np\n",
    "from openai.embeddings_utils import distances_from_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up a web crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import re\n",
    "# import urllib.request\n",
    "# from bs4 import BeautifulSoup\n",
    "# from collections import deque\n",
    "# from html.parser import HTMLParser\n",
    "# from urllib.parse import urlparse\n",
    "# import os\n",
    "\n",
    "# Regex pattern to match a URL\n",
    "HTTP_URL_PATTERN = r'^http[s]*://.+'\n",
    "\n",
    "domain = \"www.sheneller.com\" # <- domain to be crawled\n",
    "full_url = \"https://www.sheneller.com/post/the-traditional-sri-lankan-wellness-village-experience-ulpotha-retreat\" # <- domain to be crawled with https or http\n",
    "\n",
    "# Create a class to parse the HTML and get the hyperlinks\n",
    "class HyperlinkParser(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Create a list to store the hyperlinks\n",
    "        self.hyperlinks = []\n",
    "\n",
    "    # Override the HTMLParser's handle_starttag method to get the hyperlinks\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        attrs = dict(attrs)\n",
    "\n",
    "        # If the tag is an anchor tag and it has an href attribute, add the href attribute to the list of hyperlinks\n",
    "        if tag == \"a\" and \"href\" in attrs:\n",
    "            self.hyperlinks.append(attrs[\"href\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the hyperlinks from a URL\n",
    "def get_hyperlinks(url):\n",
    "    \n",
    "    # Try to open the URL and read the HTML\n",
    "    try:\n",
    "        # Open the URL and read the HTML\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "\n",
    "            # If the response is not HTML, return an empty list\n",
    "            if not response.info().get('Content-Type').startswith(\"text/html\"):\n",
    "                return []\n",
    "            \n",
    "            # Decode the HTML\n",
    "            html = response.read().decode('utf-8')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return []\n",
    "\n",
    "    # Create the HTML Parser and then Parse the HTML to get hyperlinks\n",
    "    parser = HyperlinkParser()\n",
    "    parser.feed(html)\n",
    "\n",
    "    return parser.hyperlinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the hyperlinks from a URL that are within the same domain\n",
    "def get_domain_hyperlinks(local_domain, url):\n",
    "    clean_links = []\n",
    "    for link in set(get_hyperlinks(url)):\n",
    "        clean_link = None\n",
    "\n",
    "        # If the link is a URL, check if it is within the same domain\n",
    "        if re.search(HTTP_URL_PATTERN, link):\n",
    "            # Parse the URL and check if the domain is the same\n",
    "            url_obj = urlparse(link)\n",
    "            if url_obj.netloc == local_domain:\n",
    "                clean_link = link\n",
    "\n",
    "        # If the link is not a URL, check if it is a relative link\n",
    "        else:\n",
    "            if link.startswith(\"/\"):\n",
    "                link = link[1:]\n",
    "            elif link.startswith(\"#\") or link.startswith(\"mailto:\"):\n",
    "                continue\n",
    "            clean_link = \"https://\" + local_domain + \"/\" + link\n",
    "\n",
    "        if clean_link is not None:\n",
    "            if clean_link.endswith(\"/\"):\n",
    "                clean_link = clean_link[:-1]\n",
    "            clean_links.append(clean_link)\n",
    "\n",
    "    # Return the list of hyperlinks that are within the same domain\n",
    "    return list(set(clean_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.sheneller.com/post/the-traditional-sri-lankan-wellness-village-experience-ulpotha-retreat\n"
     ]
    }
   ],
   "source": [
    "def crawl(url):\n",
    "    # Parse the URL and get the domain\n",
    "    local_domain = urlparse(url).netloc\n",
    "\n",
    "    # Create a queue to store the URLs to crawl\n",
    "    queue = deque([url])\n",
    "\n",
    "    # Create a set to store the URLs that have already been seen (no duplicates)\n",
    "    seen = set([url])\n",
    "\n",
    "    # Create a directory to store the text files\n",
    "    if not os.path.exists(\"text/\"):\n",
    "            os.mkdir(\"text/\")\n",
    "\n",
    "    if not os.path.exists(\"text/\"+local_domain+\"/\"):\n",
    "            os.mkdir(\"text/\" + local_domain + \"/\")\n",
    "\n",
    "    # Create a directory to store the csv files\n",
    "    if not os.path.exists(\"processed\"):\n",
    "            os.mkdir(\"processed\")\n",
    "\n",
    "    # While the queue is not empty, continue crawling\n",
    "    while queue:\n",
    "\n",
    "        # Get the next URL from the queue\n",
    "        url = queue.pop()\n",
    "        print(url) # for debugging and to see the progress\n",
    "\n",
    "        # Save text from the url to a <url>.txt file\n",
    "        with open('text/'+local_domain+'/'+url[8:].replace(\"/\", \"_\") + \".txt\", \"w\", encoding=\"UTF-8\") as f:\n",
    "\n",
    "            # Get the text from the URL using BeautifulSoup\n",
    "            soup = BeautifulSoup(requests.get(url).text, \"html.parser\")\n",
    "\n",
    "            # Get the text but remove the tags\n",
    "            text = soup.get_text()\n",
    "\n",
    "            # If the crawler gets to a page that requires JavaScript, it will stop the crawl\n",
    "            if (\"You need to enable JavaScript to run this app.\" in text):\n",
    "                print(\"Unable to parse page \" + url + \" due to JavaScript being required\")\n",
    "            \n",
    "            # Otherwise, write the text to the file in the text directory\n",
    "            f.write(text)\n",
    "\n",
    "        # Get the hyperlinks from the URL and add them to the queue\n",
    "#         for link in get_domain_hyperlinks(local_domain, url):\n",
    "#             if link not in seen:\n",
    "#                 queue.append(link)\n",
    "#                 seen.add(link)\n",
    "\n",
    "crawl(full_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an embeddings index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove blank empty lines\n",
    "def remove_newlines(serie):\n",
    "    serie = serie.str.replace('\\n', ' ')\n",
    "    serie = serie.str.replace('\\\\n', ' ')\n",
    "    serie = serie.str.replace('  ', ' ')\n",
    "    serie = serie.str.replace('  ', ' ')\n",
    "    return serie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>er.com</td>\n",
       "      <td>er.com.                  google.com, pub-30673...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>er.com beauty</td>\n",
       "      <td>er.com beauty.                  google.com, pu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>er.com blog</td>\n",
       "      <td>er.com blog.                  google.com, pub-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>er.com blog categories bali on a budget</td>\n",
       "      <td>er.com blog categories bali on a budget.      ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>er.com blog categories this is sri lanka</td>\n",
       "      <td>er.com blog categories this is sri lanka.     ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      fname  \\\n",
       "0                                    er.com   \n",
       "1                             er.com beauty   \n",
       "2                               er.com blog   \n",
       "3   er.com blog categories bali on a budget   \n",
       "4  er.com blog categories this is sri lanka   \n",
       "\n",
       "                                                text  \n",
       "0  er.com.                  google.com, pub-30673...  \n",
       "1  er.com beauty.                  google.com, pu...  \n",
       "2  er.com blog.                  google.com, pub-...  \n",
       "3  er.com blog categories bali on a budget.      ...  \n",
       "4  er.com blog categories this is sri lanka.     ...  "
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# Create a list to store the text files\n",
    "texts=[]\n",
    "\n",
    "# Get all the text files in the text directory\n",
    "for file in os.listdir(\"text/\" + domain + \"/\"):\n",
    "\n",
    "    # Open the file and read the text\n",
    "    with open(\"text/\" + domain + \"/\" + file, \"r\", encoding=\"UTF-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "        # Omit the first 11 lines and the last 4 lines, then replace -, _, and #update with spaces.\n",
    "        texts.append((file[11:-4].replace('-',' ').replace('_', ' ').replace('#update',''), text))\n",
    "\n",
    "# Create a dataframe from the list of texts\n",
    "df = pd.DataFrame(texts, columns = ['fname', 'text'])\n",
    "\n",
    "# Set the text column to be the raw text with the newlines removed\n",
    "df['text'] = df.fname + \". \" + remove_newlines(df.text)\n",
    "df.to_csv('processed/scraped.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import tiktoken\n",
    "\n",
    "# Load the cl100k_base tokenizer which is designed to work with the ada-002 model\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "df = pd.read_csv('processed/scraped.csv', index_col=0)\n",
    "df.columns = ['title', 'text']\n",
    "\n",
    "# Tokenize the text and save the number of tokens to a new column\n",
    "df['n_tokens'] = df.text.apply(lambda x: len(tokenizer.encode(x)))\n",
    "\n",
    "# Visualize the distribution of the number of tokens per row using a histogram\n",
    "df.n_tokens.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 500\n",
    "\n",
    "# Function to split the text into chunks of a maximum number of tokens\n",
    "def split_into_many(text, max_tokens = max_tokens):\n",
    "\n",
    "    # Split the text into sentences\n",
    "    sentences = text.split('. ')\n",
    "\n",
    "    # Get the number of tokens for each sentence\n",
    "    n_tokens = [len(tokenizer.encode(\" \" + sentence)) for sentence in sentences]\n",
    "    \n",
    "    chunks = []\n",
    "    tokens_so_far = 0\n",
    "    chunk = []\n",
    "\n",
    "    # Loop through the sentences and tokens joined together in a tuple\n",
    "    for sentence, token in zip(sentences, n_tokens):\n",
    "\n",
    "        # If the number of tokens so far plus the number of tokens in the current sentence is greater \n",
    "        # than the max number of tokens, then add the chunk to the list of chunks and reset\n",
    "        # the chunk and tokens so far\n",
    "        if tokens_so_far + token > max_tokens:\n",
    "            chunks.append(\". \".join(chunk) + \".\")\n",
    "            chunk = []\n",
    "            tokens_so_far = 0\n",
    "\n",
    "        # If the number of tokens in the current sentence is greater than the max number of \n",
    "        # tokens, go to the next sentence\n",
    "        if token > max_tokens:\n",
    "            continue\n",
    "\n",
    "        # Otherwise, add the sentence to the chunk and add the number of tokens to the total\n",
    "        chunk.append(sentence)\n",
    "        tokens_so_far += token + 1\n",
    "\n",
    "    return chunks\n",
    "    \n",
    "\n",
    "shortened = []\n",
    "\n",
    "# Loop through the dataframe\n",
    "for row in df.iterrows():\n",
    "\n",
    "    # If the text is None, go to the next row\n",
    "    if row[1]['text'] is None:\n",
    "        continue\n",
    "\n",
    "    # If the number of tokens is greater than the max number of tokens, split the text into chunks\n",
    "    if row[1]['n_tokens'] > max_tokens:\n",
    "        shortened += split_into_many(row[1]['text'])\n",
    "    \n",
    "    # Otherwise, add the text to the list of shortened texts\n",
    "    else:\n",
    "        shortened.append( row[1]['text'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(shortened, columns = ['text'])\n",
    "df['n_tokens'] = df.text.apply(lambda x: len(tokenizer.encode(x)))\n",
    "df.n_tokens.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>n_tokens</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>er.com.                  google.com, pub-30673...</td>\n",
       "      <td>304</td>\n",
       "      <td>[-0.0026199347339570522, 0.007963530719280243,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>er.com beauty.                  google.com, pu...</td>\n",
       "      <td>121</td>\n",
       "      <td>[0.00032139004906639457, 0.013038176111876965,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>er.com blog.                  google.com, pub-...</td>\n",
       "      <td>186</td>\n",
       "      <td>[0.018874598667025566, 0.02605769969522953, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>er.com blog categories bali on a budget.      ...</td>\n",
       "      <td>231</td>\n",
       "      <td>[0.011494230479001999, 0.009765672497451305, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>er.com blog categories this is sri lanka.     ...</td>\n",
       "      <td>322</td>\n",
       "      <td>[0.018257930874824524, 0.020727021619677544, -...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  n_tokens  \\\n",
       "0  er.com.                  google.com, pub-30673...       304   \n",
       "1  er.com beauty.                  google.com, pu...       121   \n",
       "2  er.com blog.                  google.com, pub-...       186   \n",
       "3  er.com blog categories bali on a budget.      ...       231   \n",
       "4  er.com blog categories this is sri lanka.     ...       322   \n",
       "\n",
       "                                          embeddings  \n",
       "0  [-0.0026199347339570522, 0.007963530719280243,...  \n",
       "1  [0.00032139004906639457, 0.013038176111876965,...  \n",
       "2  [0.018874598667025566, 0.02605769969522953, -0...  \n",
       "3  [0.011494230479001999, 0.009765672497451305, 0...  \n",
       "4  [0.018257930874824524, 0.020727021619677544, -...  "
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import openai\n",
    "openai.api_key = \"<API KEY>\"\n",
    "\n",
    "#create embeddings\n",
    "df['embeddings'] = df.text.apply(lambda x: openai.Embedding.create(input=x, engine='text-embedding-ada-002')['data'][0]['embedding'])\n",
    "\n",
    "df.to_csv('processed/embeddings.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade numpy --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Q&A system with embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>n_tokens</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>er.com.                  google.com, pub-30673...</td>\n",
       "      <td>304</td>\n",
       "      <td>[-0.0026199347339570522, 0.007963530719280243,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>er.com beauty.                  google.com, pu...</td>\n",
       "      <td>121</td>\n",
       "      <td>[0.00032139004906639457, 0.013038176111876965,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>er.com blog.                  google.com, pub-...</td>\n",
       "      <td>186</td>\n",
       "      <td>[0.018874598667025566, 0.02605769969522953, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>er.com blog categories bali on a budget.      ...</td>\n",
       "      <td>231</td>\n",
       "      <td>[0.011494230479001999, 0.009765672497451305, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>er.com blog categories this is sri lanka.     ...</td>\n",
       "      <td>322</td>\n",
       "      <td>[0.018257930874824524, 0.020727021619677544, -...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  n_tokens  \\\n",
       "0  er.com.                  google.com, pub-30673...       304   \n",
       "1  er.com beauty.                  google.com, pu...       121   \n",
       "2  er.com blog.                  google.com, pub-...       186   \n",
       "3  er.com blog categories bali on a budget.      ...       231   \n",
       "4  er.com blog categories this is sri lanka.     ...       322   \n",
       "\n",
       "                                          embeddings  \n",
       "0  [-0.0026199347339570522, 0.007963530719280243,...  \n",
       "1  [0.00032139004906639457, 0.013038176111876965,...  \n",
       "2  [0.018874598667025566, 0.02605769969522953, -0...  \n",
       "3  [0.011494230479001999, 0.009765672497451305, 0...  \n",
       "4  [0.018257930874824524, 0.020727021619677544, -...  "
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# from openai.embeddings_utils import distances_from_embeddings\n",
    "\n",
    "#Turn embeddings into a Numpy array\n",
    "df=pd.read_csv('processed/embeddings.csv', index_col=0)\n",
    "df['embeddings'] = df['embeddings'].apply(eval).apply(np.array)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install openai[embeddings] --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from openai.embeddings_utils import distances_from_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_context(\n",
    "    question, df, max_len=1800, size=\"ada\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a context for a question by finding the most similar context from the dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the embeddings for the question\n",
    "    q_embeddings = openai.Embedding.create(input=question, engine='text-embedding-ada-002')['data'][0]['embedding']\n",
    "\n",
    "    # Get the distances from the embeddings\n",
    "    df['distances'] = distances_from_embeddings(q_embeddings, df['embeddings'].values, distance_metric='cosine')\n",
    "\n",
    "\n",
    "    returns = []\n",
    "    cur_len = 0\n",
    "\n",
    "    # Sort by distance and add the text to the context until the context is too long\n",
    "    for i, row in df.sort_values('distances', ascending=True).iterrows():\n",
    "        \n",
    "        # Add the length of the text to the current length\n",
    "        cur_len += row['n_tokens'] + 4\n",
    "        \n",
    "        # If the context is too long, break\n",
    "        if cur_len > max_len:\n",
    "            break\n",
    "        \n",
    "        # Else add it to the text that is being returned\n",
    "        returns.append(row[\"text\"])\n",
    "\n",
    "    # Return the context\n",
    "    return \"\\n\\n###\\n\\n\".join(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(\n",
    "    df,\n",
    "    model=\"text-davinci-003\",\n",
    "    question=\"Am I allowed to publish model outputs to Twitter, without a human review?\",\n",
    "    max_len=1800,\n",
    "    size=\"ada\",\n",
    "    debug=False,\n",
    "    max_tokens=150,\n",
    "    stop_sequence=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Answer a question based on the most similar context from the dataframe texts\n",
    "    \"\"\"\n",
    "    context = create_context(\n",
    "        question,\n",
    "        df,\n",
    "        max_len=max_len,\n",
    "        size=size,\n",
    "    )\n",
    "    # If debug, print the raw model response\n",
    "    if debug:\n",
    "        print(\"Context:\\n\" + context)\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "    try:\n",
    "        # Create a completions using the question and context\n",
    "        response = openai.Completion.create(\n",
    "            prompt=f\"Answer the question based on the context below, and if the question can't be answered based on the context, say \\\"I don't know\\\"\\n\\nContext: {context}\\n\\n---\\n\\nQuestion: {question}\\nAnswer:\",\n",
    "            temperature=0,\n",
    "            max_tokens=max_tokens,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            stop=stop_sequence,\n",
    "            model=model,\n",
    "        )\n",
    "        return response[\"choices\"][0][\"text\"].strip()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know.\""
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question(df, question=\"What day is it?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Ulpotha retreat is a private secret hideaway located in the north-central region of Sri Lanka, within the Cultural Triangle. It offers two-week yoga retreats with some of the world's best yoga instructors, as well as a range of massages, therapies, and Ayurvedic treatments.\""
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question(df, question=\"What is Ulpotha retreat?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know.\""
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question(df, question=\"What are the packages available to stay at Ulpotha?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi!'"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question(df, question=\"Hi!\", debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This website is about travel guides, content creation, modelling portfolio, and blog posts about Sri Lanka.'"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question(df, question=\"Can you tell me what this website is about?\", debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know.\""
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question(df, question=\"Can you describe the content of the video included in the website?\", debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know.\""
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question(df, question=\"Hi! How are you?\", debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
