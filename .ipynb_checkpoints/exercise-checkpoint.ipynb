{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: aiohttp==3.8.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 1)) (3.8.3)Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: aiosignal==1.3.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: appnope==0.1.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 3)) (0.1.3)\n",
      "Requirement already satisfied: asttokens==2.2.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 4)) (2.2.1)\n",
      "Requirement already satisfied: async-timeout==4.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 5)) (4.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Cannot uninstall 'entrypoints'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: attrs==22.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 6)) (22.2.0)\n",
      "Requirement already satisfied: backcall==0.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 7)) (0.2.0)\n",
      "Requirement already satisfied: beautifulsoup4==4.11.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 8)) (4.11.1)\n",
      "Requirement already satisfied: blobfile==2.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 9)) (2.0.1)\n",
      "Requirement already satisfied: bs4==0.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 10)) (0.0.1)\n",
      "Requirement already satisfied: certifi==2022.12.7 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 11)) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer==2.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 12)) (2.1.1)\n",
      "Requirement already satisfied: comm==0.1.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 13)) (0.1.2)\n",
      "Requirement already satisfied: contourpy==1.0.7 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 14)) (1.0.7)\n",
      "Requirement already satisfied: cycler==0.11.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 15)) (0.11.0)\n",
      "Requirement already satisfied: debugpy==1.6.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 16)) (1.6.5)\n",
      "Requirement already satisfied: decorator==5.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 17)) (5.1.1)\n",
      "Requirement already satisfied: docopt==0.6.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 18)) (0.6.2)\n",
      "Collecting entrypoints==0.4\n",
      "  Using cached entrypoints-0.4-py3-none-any.whl (5.3 kB)\n",
      "Collecting executing==1.2.0\n",
      "  Using cached executing-1.2.0-py2.py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: filelock==3.9.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 21)) (3.9.0)\n",
      "Collecting fonttools==4.38.0\n",
      "  Using cached fonttools-4.38.0-py3-none-any.whl (965 kB)\n",
      "Requirement already satisfied: frozenlist==1.3.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 23)) (1.3.3)\n",
      "Collecting huggingface-hub==0.11.1\n",
      "  Using cached huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
      "Requirement already satisfied: idna==3.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 25)) (3.4)\n",
      "Collecting ipykernel==6.20.1\n",
      "  Using cached ipykernel-6.20.1-py3-none-any.whl (149 kB)\n",
      "Collecting ipython==8.8.0\n",
      "  Using cached ipython-8.8.0-py3-none-any.whl (775 kB)\n",
      "Collecting jedi==0.18.2\n",
      "  Using cached jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
      "Collecting joblib==1.2.0\n",
      "  Using cached joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "Collecting jupyter_client==7.4.8\n",
      "  Using cached jupyter_client-7.4.8-py3-none-any.whl (133 kB)\n",
      "Collecting jupyter_core==5.1.3\n",
      "  Using cached jupyter_core-5.1.3-py3-none-any.whl (93 kB)\n",
      "Collecting kiwisolver==1.4.4\n",
      "  Using cached kiwisolver-1.4.4-cp38-cp38-win_amd64.whl (55 kB)\n",
      "Requirement already satisfied: lxml==4.9.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 33)) (4.9.2)\n",
      "Collecting matplotlib==3.6.3\n",
      "  Using cached matplotlib-3.6.3-cp38-cp38-win_amd64.whl (7.2 MB)\n",
      "Collecting matplotlib-inline==0.1.6\n",
      "  Using cached matplotlib_inline-0.1.6-py3-none-any.whl (9.4 kB)\n",
      "Requirement already satisfied: multidict==6.0.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 36)) (6.0.4)\n",
      "Collecting nest-asyncio==1.5.6\n",
      "  Using cached nest_asyncio-1.5.6-py3-none-any.whl (5.2 kB)\n",
      "Collecting numpy==1.24.1\n",
      "  Using cached numpy-1.24.1-cp38-cp38-win_amd64.whl (14.9 MB)\n",
      "Processing c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\2f\\9c\\55\\95d3609ccfc463eeffb96d50c756f1f1899453b85e92021a0a\\openai-0.26.1-py3-none-any.whl\n",
      "Collecting packaging==23.0\n",
      "  Using cached packaging-23.0-py3-none-any.whl (42 kB)\n",
      "Collecting pandas==1.5.2\n",
      "  Using cached pandas-1.5.2-cp38-cp38-win_amd64.whl (11.0 MB)\n",
      "Collecting parso==0.8.3\n",
      "  Using cached parso-0.8.3-py2.py3-none-any.whl (100 kB)\n",
      "Requirement already satisfied: pexpect==4.8.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 43)) (4.8.0)\n",
      "Requirement already satisfied: pickleshare==0.7.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 44)) (0.7.5)\n",
      "Collecting Pillow==9.4.0\n",
      "  Using cached Pillow-9.4.0-cp38-cp38-win_amd64.whl (2.5 MB)\n",
      "Collecting pipreqs==0.4.11\n",
      "  Using cached pipreqs-0.4.11-py2.py3-none-any.whl (32 kB)\n",
      "Collecting platformdirs==2.6.2\n",
      "  Using cached platformdirs-2.6.2-py3-none-any.whl (14 kB)\n",
      "Collecting plotly==5.12.0\n",
      "  Using cached plotly-5.12.0-py2.py3-none-any.whl (15.2 MB)\n",
      "Collecting prompt-toolkit==3.0.36\n",
      "  Using cached prompt_toolkit-3.0.36-py3-none-any.whl (386 kB)\n",
      "Collecting psutil==5.9.4\n",
      "  Using cached psutil-5.9.4-cp36-abi3-win_amd64.whl (252 kB)\n",
      "Collecting ptyprocess==0.7.0\n",
      "  Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)\n",
      "Collecting pure-eval==0.2.2\n",
      "  Using cached pure_eval-0.2.2-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: pycryptodomex==3.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 53)) (3.17)\n",
      "Collecting Pygments==2.14.0\n",
      "  Using cached Pygments-2.14.0-py3-none-any.whl (1.1 MB)\n",
      "Collecting pyparsing==3.0.9\n",
      "  Using cached pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "Requirement already satisfied: python-dateutil==2.8.2 in c:\\users\\user\\appdata\\roaming\\python\\python38\\site-packages (from -r requirements.txt (line 56)) (2.8.2)\n",
      "Collecting pytz==2022.7.1\n",
      "  Using cached pytz-2022.7.1-py2.py3-none-any.whl (499 kB)\n",
      "Collecting PyYAML==6.0\n",
      "  Using cached PyYAML-6.0-cp38-cp38-win_amd64.whl (155 kB)\n",
      "Collecting pyzmq==24.0.1\n",
      "  Using cached pyzmq-24.0.1-cp38-cp38-win_amd64.whl (998 kB)\n",
      "Collecting regex==2022.10.31\n",
      "  Using cached regex-2022.10.31-cp38-cp38-win_amd64.whl (267 kB)\n",
      "Collecting requests==2.28.1\n",
      "  Using cached requests-2.28.1-py3-none-any.whl (62 kB)\n",
      "Collecting scikit-learn==1.2.0\n",
      "  Using cached scikit_learn-1.2.0-cp38-cp38-win_amd64.whl (8.2 MB)\n",
      "Collecting scipy==1.10.0\n",
      "  Using cached scipy-1.10.0-cp38-cp38-win_amd64.whl (42.2 MB)\n",
      "Requirement already satisfied: six==1.16.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 64)) (1.16.0)\n",
      "Requirement already satisfied: soupsieve==2.3.2.post1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 65)) (2.3.2.post1)\n",
      "Collecting stack-data==0.6.2\n",
      "  Using cached stack_data-0.6.2-py3-none-any.whl (24 kB)\n",
      "Collecting tenacity==8.1.0\n",
      "  Using cached tenacity-8.1.0-py3-none-any.whl (23 kB)\n",
      "Collecting threadpoolctl==3.1.0\n",
      "  Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting tiktoken==0.1.2\n",
      "  Using cached tiktoken-0.1.2-cp38-cp38-win_amd64.whl (575 kB)\n",
      "Collecting tokenizers==0.13.2\n",
      "  Using cached tokenizers-0.13.2-cp38-cp38-win_amd64.whl (3.3 MB)\n",
      "Collecting tornado==6.2\n",
      "  Using cached tornado-6.2-cp37-abi3-win_amd64.whl (425 kB)\n",
      "Collecting tqdm==4.64.1\n",
      "  Using cached tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: traitlets==5.8.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 73)) (5.8.1)\n",
      "Collecting transformers==4.25.1\n",
      "  Using cached transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
      "Requirement already satisfied: typing_extensions==4.4.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 75)) (4.4.0)\n",
      "Requirement already satisfied: urllib3==1.26.13 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 76)) (1.26.13)\n",
      "Requirement already satisfied: wcwidth==0.2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 77)) (0.2.5)\n",
      "Collecting yarg==0.1.9\n",
      "  Using cached yarg-0.1.9-py2.py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: yarl==1.8.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 79)) (1.8.2)\n",
      "Requirement already satisfied: colorama; sys_platform == \"win32\" in c:\\users\\user\\anaconda3\\lib\\site-packages (from ipython==8.8.0->-r requirements.txt (line 27)) (0.4.4)\n",
      "Requirement already satisfied: pywin32>=1.0; sys_platform == \"win32\" and platform_python_implementation != \"PyPy\" in c:\\users\\user\\anaconda3\\lib\\site-packages (from jupyter_core==5.1.3->-r requirements.txt (line 31)) (227)\n",
      "Installing collected packages: entrypoints, executing, fonttools, packaging, requests, PyYAML, tqdm, huggingface-hub, matplotlib-inline, nest-asyncio, pyzmq, platformdirs, jupyter-core, tornado, jupyter-client, pure-eval, stack-data, prompt-toolkit, Pygments, parso, jedi, ipython, psutil, ipykernel, joblib, kiwisolver, Pillow, numpy, pyparsing, matplotlib, openai, pytz, pandas, yarg, pipreqs, tenacity, plotly, ptyprocess, regex, threadpoolctl, scipy, scikit-learn, tiktoken, tokenizers, transformers\n",
      "  Attempting uninstall: entrypoints\n",
      "    Found existing installation: entrypoints 0.3\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import requests\n",
    "import re\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import deque\n",
    "from html.parser import HTMLParser\n",
    "from urllib.parse import urlparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import openai\n",
    "import numpy as np\n",
    "from openai.embeddings_utils import distances_from_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex pattern to match a URL\n",
    "HTTP_URL_PATTERN = r'^http[s]*://.+'\n",
    "\n",
    "domain = \"www.sheneller.com\" # <- domain to be crawled\n",
    "full_url = \"https://www.sheneller.com/post/the-traditional-sri-lankan-wellness-village-experience-ulpotha-retreat\" # <- domain to be crawled with https or http\n",
    "\n",
    "def crawl(url):\n",
    "    # Parse the URL and get the domain\n",
    "    local_domain = urlparse(url).netloc\n",
    "\n",
    "    # Create a directory to store the text files\n",
    "    if not os.path.exists(\"text/\"):\n",
    "            os.mkdir(\"text/\")\n",
    "\n",
    "    if not os.path.exists(\"text/\"+local_domain+\"/\"):\n",
    "            os.mkdir(\"text/\" + local_domain + \"/\")\n",
    "\n",
    "    # Create a directory to store the csv files\n",
    "    if not os.path.exists(\"processed\"):\n",
    "            os.mkdir(\"processed\")\n",
    "\n",
    "    # Save text from the url to a <url>.txt file\n",
    "    with open('text/'+local_domain+'/'+url[8:].replace(\"/\", \"_\") + \".txt\", \"w\", encoding=\"UTF-8\") as f:\n",
    "\n",
    "        # Get the text from the URL using BeautifulSoup\n",
    "        soup = BeautifulSoup(requests.get(url).text, \"html.parser\")\n",
    "\n",
    "        # Get the text but remove the tags\n",
    "        text = soup.get_text()\n",
    "\n",
    "        # If the crawler gets to a page that requires JavaScript, it will stop the crawl\n",
    "        if (\"You need to enable JavaScript to run this app.\" in text):\n",
    "            print(\"Unable to parse page \" + url + \" due to JavaScript being required\")\n",
    "            \n",
    "        # Otherwise, write the text to the file in the text directory\n",
    "        f.write(text)\n",
    "\n",
    "crawl(full_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove blank empty lines\n",
    "def remove_newlines(serie):\n",
    "    serie = serie.str.replace('\\n', ' ')\n",
    "    serie = serie.str.replace('\\\\n', ' ')\n",
    "    serie = serie.str.replace('  ', ' ')\n",
    "    serie = serie.str.replace('  ', ' ')\n",
    "    return serie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>er.com post the traditional sri lankan wellnes...</td>\n",
       "      <td>er.com post the traditional sri lankan wellnes...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               fname  \\\n",
       "0  er.com post the traditional sri lankan wellnes...   \n",
       "\n",
       "                                                text  \n",
       "0  er.com post the traditional sri lankan wellnes...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a list to store the text files\n",
    "texts=[]\n",
    "\n",
    "# Get all the text files in the text directory\n",
    "for file in os.listdir(\"text/\" + domain + \"/\"):\n",
    "\n",
    "    # Open the file and read the text\n",
    "    with open(\"text/\" + domain + \"/\" + file, \"r\", encoding=\"UTF-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "        # Omit the first 11 lines and the last 4 lines, then replace -, _, and #update with spaces.\n",
    "        texts.append((file[11:-4].replace('-',' ').replace('_', ' ').replace('#update',''), text))\n",
    "\n",
    "# Create a dataframe from the list of texts\n",
    "df = pd.DataFrame(texts, columns = ['fname', 'text'])\n",
    "\n",
    "# Set the text column to be the raw text with the newlines removed\n",
    "df['text'] = df.fname + \". \" + remove_newlines(df.text)\n",
    "df.to_csv('processed/scraped.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cl100k_base tokenizer which is designed to work with the ada-002 model\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "df = pd.read_csv('processed/scraped.csv', index_col=0)\n",
    "df.columns = ['title', 'text']\n",
    "\n",
    "# Tokenize the text and save the number of tokens to a new column\n",
    "df['n_tokens'] = df.text.apply(lambda x: len(tokenizer.encode(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 500\n",
    "\n",
    "# Function to split the text into chunks of a maximum number of tokens\n",
    "def split_into_many(text, max_tokens = max_tokens):\n",
    "\n",
    "    # Split the text into sentences\n",
    "    sentences = text.split('. ')\n",
    "\n",
    "    # Get the number of tokens for each sentence\n",
    "    n_tokens = [len(tokenizer.encode(\" \" + sentence)) for sentence in sentences]\n",
    "    \n",
    "    chunks = []\n",
    "    tokens_so_far = 0\n",
    "    chunk = []\n",
    "\n",
    "    # Loop through the sentences and tokens joined together in a tuple\n",
    "    for sentence, token in zip(sentences, n_tokens):\n",
    "\n",
    "        # If the number of tokens so far plus the number of tokens in the current sentence is greater \n",
    "        # than the max number of tokens, then add the chunk to the list of chunks and reset\n",
    "        # the chunk and tokens so far\n",
    "        if tokens_so_far + token > max_tokens:\n",
    "            chunks.append(\". \".join(chunk) + \".\")\n",
    "            chunk = []\n",
    "            tokens_so_far = 0\n",
    "\n",
    "        # If the number of tokens in the current sentence is greater than the max number of \n",
    "        # tokens, go to the next sentence\n",
    "        if token > max_tokens:\n",
    "            continue\n",
    "\n",
    "        # Otherwise, add the sentence to the chunk and add the number of tokens to the total\n",
    "        chunk.append(sentence)\n",
    "        tokens_so_far += token + 1\n",
    "\n",
    "    return chunks\n",
    "    \n",
    "\n",
    "shortened = []\n",
    "\n",
    "# Loop through the dataframe\n",
    "for row in df.iterrows():\n",
    "\n",
    "    # If the text is None, go to the next row\n",
    "    if row[1]['text'] is None:\n",
    "        continue\n",
    "\n",
    "    # If the number of tokens is greater than the max number of tokens, split the text into chunks\n",
    "    if row[1]['n_tokens'] > max_tokens:\n",
    "        shortened += split_into_many(row[1]['text'])\n",
    "    \n",
    "    # Otherwise, add the text to the list of shortened texts\n",
    "    else:\n",
    "        shortened.append( row[1]['text'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(shortened, columns = ['text'])\n",
    "df['n_tokens'] = df.text.apply(lambda x: len(tokenizer.encode(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>n_tokens</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>er.com post the traditional sri lankan wellnes...</td>\n",
       "      <td>423</td>\n",
       "      <td>[0.012547806836664677, 0.013542787171900272, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  n_tokens  \\\n",
       "0  er.com post the traditional sri lankan wellnes...       423   \n",
       "\n",
       "                                          embeddings  \n",
       "0  [0.012547806836664677, 0.013542787171900272, 0...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import openai\n",
    "openai.api_key = \"<API-KEY>\"\n",
    "\n",
    "#create embeddings\n",
    "df['embeddings'] = df.text.apply(lambda x: openai.Embedding.create(input=x, engine='text-embedding-ada-002')['data'][0]['embedding'])\n",
    "\n",
    "df.to_csv('processed/embeddings.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>n_tokens</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>er.com post the traditional sri lankan wellnes...</td>\n",
       "      <td>423</td>\n",
       "      <td>[0.012547806836664677, 0.013542787171900272, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  n_tokens  \\\n",
       "0  er.com post the traditional sri lankan wellnes...       423   \n",
       "\n",
       "                                          embeddings  \n",
       "0  [0.012547806836664677, 0.013542787171900272, 0...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Turn embeddings into a Numpy array\n",
    "df=pd.read_csv('processed/embeddings.csv', index_col=0)\n",
    "df['embeddings'] = df['embeddings'].apply(eval).apply(np.array)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_context(\n",
    "    question, df, max_len=1800, size=\"ada\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a context for a question by finding the most similar context from the dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the embeddings for the question\n",
    "    q_embeddings = openai.Embedding.create(input=question, engine='text-embedding-ada-002')['data'][0]['embedding']\n",
    "\n",
    "    # Get the distances from the embeddings\n",
    "    df['distances'] = distances_from_embeddings(q_embeddings, df['embeddings'].values, distance_metric='cosine')\n",
    "\n",
    "\n",
    "    returns = []\n",
    "    cur_len = 0\n",
    "\n",
    "    # Sort by distance and add the text to the context until the context is too long\n",
    "    for i, row in df.sort_values('distances', ascending=True).iterrows():\n",
    "        \n",
    "        # Add the length of the text to the current length\n",
    "        cur_len += row['n_tokens'] + 4\n",
    "        \n",
    "        # If the context is too long, break\n",
    "        if cur_len > max_len:\n",
    "            break\n",
    "        \n",
    "        # Else add it to the text that is being returned\n",
    "        returns.append(row[\"text\"])\n",
    "\n",
    "    # Return the context\n",
    "    return \"\\n\\n###\\n\\n\".join(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(\n",
    "    df,\n",
    "    model=\"text-davinci-003\",\n",
    "    question=\"Am I allowed to publish model outputs to Twitter, without a human review?\",\n",
    "    max_len=1800,\n",
    "    size=\"ada\",\n",
    "    debug=False,\n",
    "    max_tokens=150,\n",
    "    stop_sequence=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Answer a question based on the most similar context from the dataframe texts\n",
    "    \"\"\"\n",
    "    context = create_context(\n",
    "        question,\n",
    "        df,\n",
    "        max_len=max_len,\n",
    "        size=size,\n",
    "    )\n",
    "    # If debug, print the raw model response\n",
    "    if debug:\n",
    "        print(\"Context:\\n\" + context)\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "    try:\n",
    "        # Create a completions using the question and context\n",
    "        response = openai.Completion.create(\n",
    "            prompt=f\"Answer the question based on the context below, and if the question can't be answered based on the context, say \\\"I don't know\\\"\\n\\nContext: {context}\\n\\n---\\n\\nQuestion: {question}\\nAnswer:\",\n",
    "            temperature=0,\n",
    "            max_tokens=max_tokens,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            stop=stop_sequence,\n",
    "            model=model,\n",
    "        )\n",
    "        return response[\"choices\"][0][\"text\"].strip()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi!'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question(df, question=\"Hi!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ulpotha retreat is a private secret hideaway modeled after a traditional Sri Lankan village, with clay homes and communal spaces built in nature. It offers yoga retreats, massages, therapies, and Ayurvedic treatments, immersing guests in the lifestyle of an authentic Sri Lankan village as it would have been a century ago.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question(df, question=\"What is Ulpotha retreat?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know.\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question(df, question=\"What day is it?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ulpotha is located in the north-central region of Sri Lanka, within the Cultural Triangle.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question(df, question=\"Where is Ulpotha located in Sri Lanka?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You're welcome!\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question(df, question=\"Thank You!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
